{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent Neural Network Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6Bx8Ns3aBD8nmg7ZT9Jkg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThomasWit21/Master-thesis/blob/main/Recurrent_Neural_Network_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF0CLf-vMbu2"
      },
      "source": [
        "Code inspired by Ripamonti(2018):\n",
        "https://www.kaggle.com/paoloripamonti/twitter-sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sDMU13XCfSR"
      },
      "source": [
        "#Decoding integers to labels\n",
        "decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\n",
        "def decode_sentiment(label):\n",
        "    return decode_map[int(label)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH5G-uV7Clmz"
      },
      "source": [
        "##Setting parameters\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# WORD2VEC \n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCImlesFMBoG"
      },
      "source": [
        "#Loading in the training data\n",
        "Vadercheck = pd.read_csv(\"/content/drive/My Drive/VADERcheck.csv\", encoding = \"ISO-8859-1\")\n",
        "Vadercheck = Vadercheck.drop(['1467810369', 'Mon Apr 06 22:19:45 PDT 2009', 'NO_QUERY', '_TheSpecialOne_'], axis = 1)\n",
        "Vadercheck = Vadercheck.rename(columns = {'0' : 'Sentiment Score', \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\" : \"Text\"})\n",
        "Samplecheck = Vadercheck.sample(n=500000)\n",
        "Samplecheck['Sentiment Score'] = Samplecheck['Sentiment Score'].apply(lambda x: decode_sentiment(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnIKs4X4DD80"
      },
      "source": [
        "#Preprocessing\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbGPKfd6DMC-"
      },
      "source": [
        "#Train, Test, Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_test = train_test_split(Samplecheck, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1am2bW3DTOk"
      },
      "source": [
        "%%time\n",
        "files = [_text.split() for _text in df_train.Text] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdLYRQivDXyo"
      },
      "source": [
        "#Word2Vec and building the vocabulary\n",
        "import gensim\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n",
        "                                            window=W2V_WINDOW, \n",
        "                                            min_count=W2V_MIN_COUNT, \n",
        "                                            workers=8)\n",
        "\n",
        "w2v_model.build_vocab(files)\n",
        "\n",
        "words = w2v_model.wv.vocab.keys()\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6mxYNSMDiUI"
      },
      "source": [
        "#Training the Word2Vec model\n",
        "%%time\n",
        "w2v_model.train(files, total_examples=len(files), epochs=W2V_EPOCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaNx_iRiDozB"
      },
      "source": [
        "#Tokenizing and padding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train.Text)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Total words\", vocab_size)\n",
        "\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.Text), maxlen=SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.Text), maxlen=SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP6aCrLqDvrb"
      },
      "source": [
        "#Making labels\n",
        "labels = df_train['Sentiment Score'].unique().tolist()\n",
        "labels.append(NEUTRAL)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc2XVuYbD6BT"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_train['Sentiment Score'].tolist())\n",
        "\n",
        "y_train = encoder.transform(df_train['Sentiment Score'].tolist())\n",
        "y_test = encoder.transform(df_test['Sentiment Score'].tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "\n",
        "print(\"y_train\",y_train.shape)\n",
        "print(\"y_test\",y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrYtEGbdD9vl"
      },
      "source": [
        "#Building the recurrent neural network model\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word in w2v_model.wv:\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enn31b97EMXn"
      },
      "source": [
        "#Training the model\n",
        "%%time\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NgIWhZYWJxM"
      },
      "source": [
        "#Evaluating the model\n",
        "score = model.evaluate(x_test, y_test)\n",
        "print()\n",
        "print(\"ACCURACY:\",score[1])\n",
        "print(\"LOSS:\",score[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}